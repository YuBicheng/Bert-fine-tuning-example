{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9142c06b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer,BertForSequenceClassification,TrainingArguments,Trainer,get_linear_schedule_with_warmup\n",
    "from torch.utils.data import Dataset,TensorDataset, random_split\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "#设置gpu 没有就是cpu\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "#固定所有的随机数的种子\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "#该句不需要gpu就可以运行\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26210358",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#Tokenizer和Bert加载\n",
    "#请从huggingface下载对应的模型，并保存在同目录的bert-base-chinese文件夹下，如果你能联网自动下载当我没说\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./bert-base-chinese\")\n",
    "bert = BertForSequenceClassification.from_pretrained(\n",
    "    \"./bert-base-chinese\",\n",
    "    num_labels=1,\n",
    "    #模型是否返回Attention的权重（score？）\n",
    "    output_attentions=False,\n",
    "    #模型是否返回全部隐藏层的状态\n",
    "    output_hidden_states = False)\n",
    "bert = bert.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d6687c",
   "metadata": {},
   "source": [
    "数据基础处理及切分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aaee0c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7766, 64]) torch.Size([7766, 64]) torch.Size([7766])\n",
      "训练集大小： 6989   测试集大小: 777\n"
     ]
    }
   ],
   "source": [
    "with open(\"./ChnSentiCorp.txt\",'r',encoding='utf8') as f:\n",
    "    data = f.readlines()\n",
    "label = []\n",
    "X = []\n",
    "for line in data:\n",
    "    label_line = int(line.split(',')[0])\n",
    "    x_line = line[2:]\n",
    "    x_line = re.sub('\\s','',x_line)\n",
    "    X.append(x_line)\n",
    "    label.append(label_line)\n",
    "#print(len(label),len(X))\n",
    "#将所有的文本转化为id 和获取对应的掩码 \n",
    "X_ = tokenizer(X,padding=True,truncation=True,max_length = 64)\n",
    "X_ids = X_['input_ids']\n",
    "X_mask = X_['attention_mask']\n",
    "X_ids = torch.LongTensor(X_ids)\n",
    "X_mask = torch.LongTensor(X_mask)\n",
    "label = torch.Tensor(label)\n",
    "print(X_ids.shape,X_mask.shape,label.shape)\n",
    "dataset = TensorDataset(X_ids,X_mask,label)\n",
    "train_size = int(0.9*len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset,[train_size,val_size])\n",
    "print(\"训练集大小：\",train_size,\"  测试集大小:\",val_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7767042c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    sampler = RandomSampler(train_dataset),\n",
    "    batch_size=batch_size)\n",
    "validation_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    sampler=SequentialSampler(val_dataset),\n",
    "    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "179bfac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#正确率计算函数\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.where(preds > 0.5, 1, 0).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1fc4018a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:18,  4.58s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 28\u001b[0m\n\u001b[0;32m     26\u001b[0m logits \u001b[38;5;241m=\u001b[39m output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     27\u001b[0m total_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m---> 28\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m#梯度裁剪 防止梯度爆炸\u001b[39;00m\n\u001b[0;32m     30\u001b[0m nn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(bert\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m1.0\u001b[39m)\n",
      "File \u001b[1;32mD:\\programing\\ANACONDA\\envs\\newpytorch\\lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\programing\\ANACONDA\\envs\\newpytorch\\lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 4\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "optimizer = AdamW(bert.parameters(),lr=2e-5,eps=1e-8)\n",
    "#一个非常方便的自动改变学习率的函数\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps)\n",
    "#不需要损失函数，模型的前向传播中里已经有了MSE损失函数，直接将所有数据丢进去就行了\n",
    "#当标签数>1时会使用交叉熵\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "for i in range(epochs):\n",
    "    start = time.time()\n",
    "    total_train_loss = 0\n",
    "    bert.train()\n",
    "    for batch_i,batch in tqdm(enumerate(train_dataloader)):\n",
    "        #训练数据加载到正确设备\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_label = batch[2].to(device)\n",
    "        #重置所有的梯度\n",
    "        bert.zero_grad()\n",
    "        output = bert.forward(b_input_ids,attention_mask=b_input_mask,labels=b_label)\n",
    "        #print(output)\n",
    "        loss = output['loss']\n",
    "        logits = output[\"logits\"]\n",
    "        total_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        #梯度裁剪 防止梯度爆炸\n",
    "        nn.utils.clip_grad_norm_(bert.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        #自动改变学习率\n",
    "        scheduler.step()\n",
    "    average_loss = total_train_loss / len(train_dataloader)\n",
    "    print(\"训练***代数:{},耗时:{},平均损失:{}\".format(i+1,time.time()-start,average_loss))\n",
    "    #开始运行验证集\n",
    "    bert.eval()\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "    start = time.time()\n",
    "    for batch in validation_dataloader:\n",
    "        #测试数据加载到正确设备\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        #验证时不需要计算梯度\n",
    "        with torch.no_grad():\n",
    "            output = bert.forward(b_input_ids,attention_mask=b_input_mask,labels=b_labels)\n",
    "        loss = output['loss']\n",
    "        logits = output[\"logits\"]\n",
    "        total_eval_loss += loss.item()\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    avg_val_loss = total_eval_loss/len(validation_dataloader)\n",
    "    print(\"验证***代数:{},耗时:{},平均损失:{},正确率:{}\".format(i+1,time.time()-start,avg_val_loss,avg_val_accuracy))\n",
    "print(\"训练结束\")    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9801aa93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newpytorch",
   "language": "python",
   "name": "newpytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
