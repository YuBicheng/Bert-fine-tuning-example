{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9142c06b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer,BertForSequenceClassification,TrainingArguments,Trainer,get_linear_schedule_with_warmup\n",
    "from torch.utils.data import Dataset,TensorDataset, random_split\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "#设置gpu 没有就是cpu\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "#固定所有的随机数的种子\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "#该句不需要gpu就可以运行\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "26210358",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#Tokenizer和Bert加载\n",
    "#请从huggingface下载对应的模型，并保存在同目录的bert-base-chinese文件夹下，如果你能联网自动下载当我没说\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./bert-base-chinese\")\n",
    "bert = BertForSequenceClassification.from_pretrained(\n",
    "    \"./bert-base-chinese\",\n",
    "    num_labels=1,\n",
    "    #模型是否返回Attention的权重（score？）\n",
    "    output_attentions=False,\n",
    "    #模型是否返回全部隐藏层的状态\n",
    "    output_hidden_states = False)\n",
    "bert = bert.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d6687c",
   "metadata": {},
   "source": [
    "数据基础处理及切分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aaee0c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([399, 64]) torch.Size([399, 64]) torch.Size([399])\n",
      "训练集大小： 359   测试集大小: 40\n"
     ]
    }
   ],
   "source": [
    "with open(\"./ChnSentiCorp.txt\",'r',encoding='utf8') as f:\n",
    "    data = f.readlines()\n",
    "label = []\n",
    "X = []\n",
    "for line in data:\n",
    "    label_line = int(line.split(',')[0])\n",
    "    x_line = line[2:]\n",
    "    x_line = re.sub('\\s','',x_line)\n",
    "    X.append(x_line)\n",
    "    label.append(label_line)\n",
    "#print(len(label),len(X))\n",
    "#将所有的文本转化为id 和获取对应的掩码 \n",
    "X_ = tokenizer(X,padding=True,truncation=True,max_length = 64)\n",
    "X_ids = X_['input_ids']\n",
    "X_mask = X_['attention_mask']\n",
    "X_ids = torch.LongTensor(X_ids)\n",
    "X_mask = torch.LongTensor(X_mask)\n",
    "label = torch.Tensor(label)\n",
    "print(X_ids.shape,X_mask.shape,label.shape)\n",
    "dataset = TensorDataset(X_ids,X_mask,label)\n",
    "train_size = int(0.9*len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset,[train_size,val_size])\n",
    "print(\"训练集大小：\",train_size,\"  测试集大小:\",val_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7767042c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    sampler = RandomSampler(train_dataset),\n",
    "    batch_size=batch_size)\n",
    "validation_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    sampler=SequentialSampler(val_dataset),\n",
    "    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "179bfac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#正确率计算函数\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.where(preds > 0.5, 1, 0).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1fc4018a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12it [00:43,  3.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练***代数:1,耗时:43.33224415779114,平均损失:0.5440009993811449\n",
      "验证***代数:1,耗时:1.5003137588500977,平均损失:0.10309580713510513,正确率:0.90625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12it [00:43,  3.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练***代数:2,耗时:43.845633029937744,平均损失:0.15290648924807707\n",
      "验证***代数:2,耗时:1.4780478477478027,平均损失:0.07184994965791702,正确率:0.90625\n",
      "训练结束\n"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "optimizer = AdamW(bert.parameters(),lr=2e-5,eps=1e-8)\n",
    "#一个非常方便的自动改变学习率的函数\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps)\n",
    "#不需要损失函数，模型的前向传播中里已经有了MSE损失函数，直接将所有数据丢进去就行了\n",
    "#当标签数>1时会使用交叉熵\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "for i in range(epochs):\n",
    "    start = time.time()\n",
    "    total_train_loss = 0\n",
    "    bert.train()\n",
    "    for batch_i,batch in tqdm(enumerate(train_dataloader)):\n",
    "        #训练数据加载到正确设备\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_label = batch[2].to(device)\n",
    "        #重置所有的梯度\n",
    "        bert.zero_grad()\n",
    "        output = bert.forward(b_input_ids,attention_mask=b_input_mask,labels=b_label)\n",
    "        #print(output)\n",
    "        loss = output['loss']\n",
    "        logits = output[\"logits\"]\n",
    "        total_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        #梯度裁剪 防止梯度爆炸\n",
    "        nn.utils.clip_grad_norm_(bert.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        #自动改变学习率\n",
    "        scheduler.step()\n",
    "    average_loss = total_train_loss / len(train_dataloader)\n",
    "    print(\"训练***代数:{},耗时:{},平均损失:{}\".format(i+1,time.time()-start,average_loss))\n",
    "    #开始运行验证集\n",
    "    bert.eval()\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "    start = time.time()\n",
    "    for batch in validation_dataloader:\n",
    "        #测试数据加载到正确设备\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        #验证时不需要计算梯度\n",
    "        with torch.no_grad():\n",
    "            output = bert.forward(b_input_ids,attention_mask=b_input_mask,labels=b_labels)\n",
    "        loss = output['loss']\n",
    "        logits = output[\"logits\"]\n",
    "        total_eval_loss += loss.item()\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    avg_val_loss = total_eval_loss/len(validation_dataloader)\n",
    "    print(\"验证***代数:{},耗时:{},平均损失:{},正确率:{}\".format(i+1,time.time()-start,avg_val_loss,avg_val_accuracy))\n",
    "print(\"训练结束\")    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9801aa93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newpytorch",
   "language": "python",
   "name": "newpytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
